{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSys coursework 2020 template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liu-Molin/UoG_Msc_DataScience/blob/master/RecSys/RecSys_coursework_2020_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THQFNe3zdt1f",
        "colab_type": "text"
      },
      "source": [
        "# Assessed Coursework Template Notebook\n",
        "\n",
        "This is the template notebook for the RecSys(H) 2020 coursework. It deals with data preparation and evaluation only.\n",
        "\n",
        "Please note:\n",
        " - use H1 text headings for grouping together blocks of cells. You can then hide these while working on other blocks\n",
        " - leave the cell output visible when you submit the notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww--_kl9-ndn",
        "colab_type": "text"
      },
      "source": [
        "## Setup block\n",
        "\n",
        "Setup the data files, Python etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFgYpbhh0tkX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b5fdcbaf-9d8f-4925-f84b-2b76864fee0d"
      },
      "source": [
        "!rm -rf ratings* books* to_read* test*\n",
        "\n",
        "!curl -o ratings.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-ratings.csv\" \n",
        "!curl -o books.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-books.csv\"\n",
        "!curl -o to_read.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-to_read.csv\"\n",
        "!curl -o test.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-test.csv\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 7631k  100 7631k    0     0  6729k      0  0:00:01  0:00:01 --:--:-- 6729k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2366k  100 2366k    0     0  2707k      0 --:--:-- --:--:-- --:--:-- 2704k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 7581k  100 7581k    0     0  6686k      0  0:00:01  0:00:01 --:--:-- 6686k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1895k  100 1895k    0     0  2049k      0 --:--:-- --:--:-- --:--:-- 2046k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpVnNrZ1EiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3f8857a0-cd0e-44c0-e189-eb5408da5a36"
      },
      "source": [
        "#Standard setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "!pip install git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight\n",
        "from spotlight.interactions import Interactions\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spotlight\n",
            "  Cloning https://github.com/cmacdonald/spotlight.git (to revision master) to /tmp/pip-install-hsh7cp6o/spotlight\n",
            "  Running command git clone -q https://github.com/cmacdonald/spotlight.git /tmp/pip-install-hsh7cp6o/spotlight\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spotlight) (1.4.0)\n",
            "Building wheels for collected packages: spotlight\n",
            "  Building wheel for spotlight (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spotlight: filename=spotlight-0.1.6-cp36-none-any.whl size=34096 sha256=bf4d1878d493a6bf72e5b6b7ce722a21ba02e0a2cbf4738e5f25c90c41ea04d6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dwamzkkt/wheels/22/6f/f1/68cc6c5b563e78737e4a8fed63ddc105a3baf25d2abccae0c6\n",
            "Successfully built spotlight\n",
            "Installing collected packages: spotlight\n",
            "Successfully installed spotlight-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJO0e0m-hun",
        "colab_type": "text"
      },
      "source": [
        "# data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKAb25iw1MYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load in the csv files\n",
        "ratings_df = pd.read_csv(\"ratings.csv\")\n",
        "books_df = pd.read_csv(\"books.csv\")\n",
        "to_read_df = pd.read_csv(\"to_read.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6rqfn53OhDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cut down the number of items and users\n",
        "counts=ratings_df[ratings_df[\"book_id\"] < 2000].groupby([\"book_id\"]).count().reset_index()\n",
        "valid_books=counts[counts[\"user_id\"] >= 10][[\"book_id\"]]\n",
        "\n",
        "books_df = books_df.merge(valid_books, on=\"book_id\")\n",
        "\n",
        "ratings_df = ratings_df[ratings_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "to_read_df = to_read_df[to_read_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
        "test = test[test[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7cgXhmYUXIn",
        "colab_type": "text"
      },
      "source": [
        "Here we construct the Interactions objects from `ratings.csv`, `to_read.csv` and `test.csv`. We manually specify the num_users and num_items parameters to all Interaction objects, in case the test set differs from your training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ClgJOdTTt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e81851fd-3a55-43cb-ee11-7eb039c6a26c"
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "iid_map = defaultdict(count().__next__)\n",
        "\n",
        "\n",
        "rating_iids = np.array([iid_map[iid] for iid in ratings_df[\"book_id\"].values], dtype = np.int32)\n",
        "test_iids = np.array([iid_map[iid] for iid in test[\"book_id\"].values], dtype = np.int32)\n",
        "toread_iids = np.array([iid_map[iid] for iid in to_read_df[\"book_id\"].values], dtype = np.int32)\n",
        "\n",
        "\n",
        "uid_map = defaultdict(count().__next__)\n",
        "test_uids = np.array([uid_map[uid] for uid in test[\"user_id\"].values], dtype = np.int32)\n",
        "rating_uids = np.array([uid_map[uid] for uid in ratings_df[\"user_id\"].values], dtype = np.int32)\n",
        "toread_uids = np.array([uid_map[iid] for iid in to_read_df[\"user_id\"].values], dtype = np.int32)\n",
        "\n",
        "\n",
        "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
        "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
        "\n",
        "\n",
        "rating_dataset = Interactions(user_ids=rating_uids,\n",
        "                               item_ids=rating_iids,\n",
        "                               ratings=ratings_df[\"rating\"].values,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "toread_dataset = Interactions(user_ids=toread_uids,\n",
        "                               item_ids=toread_iids,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "test_dataset = Interactions(user_ids=test_uids,\n",
        "                               item_ids=test_iids,\n",
        "                               num_users=len(uid_rev_map),\n",
        "                               num_items=len(iid_rev_map))\n",
        "\n",
        "print(rating_dataset)\n",
        "print(toread_dataset)\n",
        "print(test_dataset)\n",
        "\n",
        "#here we define the validation set\n",
        "toread_dataset_train, validation = random_train_test_split(toread_dataset, random_state=np.random.seed(42))\n",
        "\n",
        "num_items = test_dataset.num_items\n",
        "num_users = test_dataset.num_users"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Interactions dataset (1999 users x 1826 items x 124762 interactions)>\n",
            "<Interactions dataset (1999 users x 1826 items x 135615 interactions)>\n",
            "<Interactions dataset (1999 users x 1826 items x 33917 interactions)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt4I2C5DTUL5",
        "colab_type": "text"
      },
      "source": [
        "#Example code\n",
        "\n",
        "To evaluate soem of your hand-implemented recommender systems (e.g. Q1, Q4), you will need to instantiate objects that match the specification of a Spotlight model, which `mrr_score()` expects.\n",
        "\n",
        "\n",
        "Here is an example recommender object that returns 0 for each item, regardless of user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eaxy_hakbC",
        "colab_type": "code",
        "outputId": "8cc8efbe-4c6c-438f-c131-deb6fa917de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from spotlight.evaluation import mrr_score\n",
        "\n",
        "\n",
        "class dummymodel:\n",
        "  \n",
        "  def __init__(self, numitems):\n",
        "    self.predictions=np.zeros(numitems)\n",
        "  \n",
        "  #uid is the user we are requesting recommendations for;\n",
        "  #returns an array of scores, one for each item\n",
        "  def predict(self, uid):\n",
        "    #this model returns all zeros, regardless of userid\n",
        "    return( self.predictions )\n",
        "\n",
        "#lets evaluate how the effeciveness of dummymodel\n",
        "\n",
        "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100).mean())\n",
        "#as expected, a recommendation model that gives 0 scores for all items obtains a MRR score of 0\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQTJOmS5dB3i",
        "colab_type": "code",
        "outputId": "bd2b337e-1f24-4a2e-ab26-cf33479ed55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "#note that the latest copy of Craig's Spotlight displays a progress bar if you set verbose=True\n",
        "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1999it [00:00, 2891.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyvGgW_3ZjLV",
        "colab_type": "text"
      },
      "source": [
        "#Q1\n",
        "\n",
        "Implement the following four baselines for ranking books based on their statistics:\n",
        "- Average rating, obtained from `ratings.csv`.\n",
        "- Number of ratings, obtained from `books.csv`.\n",
        "- Number of 5* ratings, obtained from `books.csv`\n",
        "- Fraction of 5* ratings, calculated from the two evidence above.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-kNCBeDZA_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "aa787f4d-bc15-493d-ef22-15339eae4764"
      },
      "source": [
        "ratings_df.describe()\n",
        "# Average rating\n",
        "avg_rating = ratings_df.rating.mean()\n",
        "# Number of ratings\n",
        "num_ratings = books_df.ratings_count.sum()\n",
        "# Number of 5* ratings\n",
        "num_5stars = books_df.ratings_5.sum()\n",
        "# Fraction of 5* ratings\n",
        "frac_5starts = num_5stars/num_ratings\n",
        "print(frac_5starts)\n",
        "\n",
        "ratings_means = ratings_df.groupby('book_id')['rating'].mean()\n",
        "print(ratings_means.shape)\n",
        "print(ratings_means)\n",
        "\n",
        "#print(ratings_df)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4470821153024418\n",
            "(1826,)\n",
            "book_id\n",
            "1       4.149593\n",
            "2       4.018149\n",
            "3       3.224181\n",
            "4       4.322734\n",
            "5       3.756800\n",
            "          ...   \n",
            "1990    3.968750\n",
            "1991    3.900000\n",
            "1993    3.052632\n",
            "1997    3.700000\n",
            "1999    3.688889\n",
            "Name: rating, Length: 1826, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ckEhaCoGG9N",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0MpkqtIdgOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spotlight.evaluation import mrr_score\n",
        "\n",
        "\n",
        "class avg_model:\n",
        "  \n",
        "  def __init__(self, numitems):\n",
        "    self.predictions=np.zeros(numitems)\n",
        "  \n",
        "  #uid is the user we are requesting recommendations for;\n",
        "  #returns an array of scores, one for each item\n",
        "  def predict(self, uid):\n",
        "    #this model returns all zeros, regardless of userid\n",
        "    return( self.predictions )\n",
        "\n",
        "#lets evaluate how the effeciveness of dummymodel\n",
        "\n",
        "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100).mean())\n",
        "#as expected, a recommendation model that gives 0 scores for all items obtains a MRR score of 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrlcxsDdiE6t",
        "colab_type": "text"
      },
      "source": [
        "# Q2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giMqb5GUghlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}